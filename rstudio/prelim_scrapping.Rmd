---
title: "prelim_scrapping"
author: "Grace Gao"
date: "12/14/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Load library packages

```{r}
library(tidyverse)
library(rvest)
```


##Bass Connections Project

##Scrapping the URLs
Scrap all the URLs for every artist in the whole Database

#### Import a webpage using `read_html`

```{r get_pageoneurl}
results <- read_html("http://www.vondel.humanities.uva.nl/ecartico/persons/index.php?subtask=browse")
```

```{r display_htmlpage}
#displaying the html page
#results
```

### Getting text of first page
Quick Info:
li = list node in HTML
html_text: Extract attributes, text and tag name from HTML.

```{r extract_pageone_text}
names <- results %>% 
  html_nodes("li") %>% 
  html_text()

#names
```

### Parsing to get the individual links
href: <a href="https://www.w3schools.com">Visit W3Schools.com!</a>
href allows you to click on the artist name and be redirected to their page

```{r text_url_pageone}
url <- results %>% 
  html_nodes("li a") %>% 
  html_attr("href")

url #list of individual links from page one
```

### Create a tibble

Puts the vectors of names and url into a tibble to be easily accesible

```{r tibble_pageone}
results_df <- tibble(names, url)

results_df
```
This section we mutate the urls in the dataset to represent the actual urls (utilizing glue and mutate)

```{r mutate_url}
results_df <- results_df %>% 
  mutate(base_url = "http://www.vondel.humanities.uva.nl") %>% 
  mutate(full_url = glue::glue("{base_url}{url}")) %>% 
  mutate(full_url = str_replace_all(full_url, "\\.\\.", ""))

results_df
```

### Parsing through all the pages 

Get the different navigation: parsing the text
```{r pages_text}
results %>%  #results = page one
  html_nodes("div.subnav a") %>% 
  html_text()
```

Put the text into a vector and convert into tibble. However, this only gets pages 2,3,4,5 and 22
```{r loop}
navigation <- results %>% 
  html_nodes("div.subnav a") %>% 
  html_attr("href")

navigation

#makes navigation into a tibble
nav_df <- tibble(navigation)
nav_df
```

### Capture all the Pages
We will use a standardized format to get all the navigation pages as opposed to just 5. So, we extract this format from our previously captured urls.
```{r}
nav_df <- nav_df %>%
  filter(str_detect(navigation, "&page=")) %>%
  distinct(navigation) %>%
  mutate(page_no = str_extract(navigation, "\\d+$")) %>%
  mutate(page_no = as.numeric(page_no))

#nav_df
```

Utilize the standard format to get all the different navigation pages (all placed into a tibble)
```{r}
nav_df <- nav_df %>% 
  mutate(navigation = str_extract(navigation, ".*(?=\\=\\d+$)")) %>% 
  mutate(page_no = as.integer(str_replace(page_no, "^2$", "1"))) %>% 
  expand(navigation, page_no = full_seq(page_no, 1)) %>% 
  transmute(url = glue::glue("http://www.vondel.humanities.uva.nl{navigation}={page_no}"))

#nav_df
```

### Iteration

```{r}
nav_results_list <- tibble(
  html_results = map(nav_df$url[1:3],
    ~ {
      #url[1:3] - limiting to the first three summary results pages (each page = 50 results)
      Sys.sleep(2)
      # DO THIS!  sleep 2 will pause 2 seconds between server requests to avoid being identified and potentially blocked by my target web server that might see my crawling bot as a DNS attack.
      .x %>%
        read_html()
    }),
  summary_url = nav_df$url[1:3]
)

nav_results_list
```

```{r}
results_by_page <- tibble(summary_url = map_chr(nav_results_list$summary_url, as.character),
                          uri =
                            map(nav_results_list$html_results,
                                ~ .x %>%
                                  html_nodes("ul li a") %>%
                                  html_attr("href")),
                          name =
                            map(nav_results_list$html_results,
                                ~ .x %>%
                                  html_nodes("ul li a") %>%
                                  html_text()
                                )
                          )

results_by_page

results_by_page %>% 
  unnest(cols = c(uri, name)) %>% 
  filter(!str_detect(name, "ECARTICO")) %>% 
  filter(!str_detect(name, "^\\+"))
  
```